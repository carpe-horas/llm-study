{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885ca0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- deepseek-ai/DeepSeek-V3-0324:\n",
      "✅ 입력 문장: 내일도 오늘과 똑같은 하루일까?\n",
      "🔢 Token 개수: 17\n",
      "🔠 Token 리스트: ['ëĤ´', 'ìĿ¼', 'ëıĦ', 'Ġìĺ¤', 'ëĬĺ', 'ê³¼', 'Ġë', 'ĺ', 'ĳ', 'ê°', 'Ļ', 'ìĿĢ', 'Ġíķĺ', 'ë£¨', 'ìĿ¼', 'ê¹Į', '?']\n",
      "🧬 Input IDs: [35076, 14304, 9260, 30803, 92855, 9862, 1525, 249, 242, 4598, 250, 7180, 17454, 43624, 14304, 30939, 33]\n",
      "\n",
      "- Qwen/QwQ-32B:\n",
      "✅ 입력 문장: 내일도 오늘과 똑같은 하루일까?\n",
      "🔢 Token 개수: 14\n",
      "🔠 Token 리스트: ['ëĤ´', 'ìĿ¼', 'ëıĦ', 'Ġìĺ¤ëĬĺ', 'ê³¼', 'Ġëĺ', 'ĳ', 'ê°Ļ', 'ìĿĢ', 'Ġíķĺ', 'ë£¨', 'ìĿ¼', 'ê¹Į', '?']\n",
      "🧬 Input IDs: [95218, 32077, 47985, 133857, 53680, 125639, 239, 131380, 33704, 53900, 126746, 32077, 124667, 30]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_with_model(model_name, text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\n- {model_name}:\")\n",
    "    print(\"✅ 입력 문장:\", text)\n",
    "    print(\"🔢 Token 개수:\", len(input_ids))\n",
    "    print(\"🔠 Token 리스트:\", tokens)\n",
    "    print(\"🧬 Input IDs:\", input_ids)\n",
    "\n",
    "text = \"내일도 오늘과 똑같은 하루일까?\"\n",
    "\n",
    "# 원하는 모델 이름을 여기에 입력\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n",
    "tokenize_with_model(\"Qwen/QwQ-32B\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d2ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- deepseek-ai/DeepSeek-V3-0324:\n",
      "✅ 입력 문장: 내일도 오늘과 똑같은 하루일까? 오늘보다 즐거웠으면 좋겠다!!^^\n",
      "🔧 Tokenizer 종류: LlamaTokenizerFast\n",
      "🔢 Token 개수: 31\n",
      "🔠 Token 리스트: ['ëĤ´', 'ìĿ¼', 'ëıĦ', 'Ġìĺ¤', 'ëĬĺ', 'ê³¼', 'Ġë', 'ĺ', 'ĳ', 'ê°', 'Ļ', 'ìĿĢ', 'Ġíķĺ', 'ë£¨', 'ìĿ¼', 'ê¹Į', '?', 'Ġìĺ¤', 'ëĬĺ', 'ë³´ëĭ¤', 'Ġì¦', 'Ĳ', 'ê±°', 'ìĽ', 'ł', 'ìľ¼ë©´', 'Ġì¢ĭ', 'ê²ł', 'ëĭ¤', '!!', '^^']\n",
      "🧬 Input IDs: [35076, 14304, 9260, 30803, 92855, 9862, 1525, 249, 242, 4598, 250, 7180, 17454, 43624, 14304, 30939, 33, 30803, 92855, 62811, 37589, 241, 28058, 10204, 257, 80726, 61912, 70096, 3874, 6909, 29670]\n",
      "🔍 사람이 읽을 수 있는 Token들: ['내', '일', '도', ' 오', '늘', '과', ' �', '�', '�', '�', '�', '은', ' 하', '루', '일', '까', '?', ' 오', '늘', '보다', ' �', '�', '거', '�', '�', '으면', ' 좋', '겠', '다', '!!', '^^']\n",
      "💰 예상 토큰 비용 (USD): $0.00006200\n",
      "\n",
      "- Qwen/QwQ-32B:\n",
      "✅ 입력 문장: 내일도 오늘과 똑같은 하루일까? 오늘보다 즐거웠으면 좋겠다!!^^\n",
      "🔧 Tokenizer 종류: Qwen2TokenizerFast\n",
      "🔢 Token 개수: 25\n",
      "🔠 Token 리스트: ['ëĤ´', 'ìĿ¼', 'ëıĦ', 'Ġìĺ¤ëĬĺ', 'ê³¼', 'Ġëĺ', 'ĳ', 'ê°Ļ', 'ìĿĢ', 'Ġíķĺ', 'ë£¨', 'ìĿ¼', 'ê¹Į', '?', 'Ġìĺ¤ëĬĺ', 'ë³´ëĭ¤', 'Ġì¦', 'Ĳ', 'ê±°', 'ìĽł', 'ìľ¼ë©´', 'Ġì¢ĭ', 'ê²łëĭ¤', '!!', '^^']\n",
      "🧬 Input IDs: [95218, 32077, 47985, 133857, 53680, 125639, 239, 131380, 33704, 53900, 126746, 32077, 124667, 30, 133857, 129885, 132376, 238, 92192, 144035, 89940, 125953, 127816, 2928, 21876]\n",
      "🔍 사람이 읽을 수 있는 Token들: ['내', '일', '도', ' 오늘', '과', ' �', '�', '같', '은', ' 하', '루', '일', '까', '?', ' 오늘', '보다', ' �', '�', '거', '웠', '으면', ' 좋', '겠다', '!!', '^^']\n",
      "💰 예상 토큰 비용 (USD): $0.00005000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_with_model(model_name, text, cost_per_token=0.000002):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in input_ids]\n",
    "    estimated_cost = len(input_ids) * cost_per_token\n",
    "\n",
    "    print(f\"\\n- {model_name}:\")\n",
    "    print(\"✅ 입력 문장:\", text)\n",
    "    print(\"🔧 Tokenizer 종류:\", tokenizer.__class__.__name__)\n",
    "    print(\"🔢 Token 개수:\", len(input_ids))\n",
    "    print(\"🔠 Token 리스트:\", tokens)\n",
    "    print(\"🧬 Input IDs:\", input_ids)\n",
    "    print(\"🔍 사람이 읽을 수 있는 Token들:\", decoded_tokens)\n",
    "    print(f\"💰 예상 토큰 비용 (USD): ${estimated_cost:.8f}\")\n",
    "\n",
    "# 공통 입력 문장\n",
    "text = \"내일도 오늘과 똑같은 하루일까? 오늘보다 즐거웠으면 좋겠다!!^^\"\n",
    "\n",
    "# 비교할 모델\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n",
    "tokenize_with_model(\"Qwen/QwQ-32B\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a888f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model: gpt2\n",
      "⭐ 입력 문장: 2025년 새해 福 많이 받으세요!! Happy New Year~~\n",
      "✅ Token 수: 35\n",
      "✅ Token ID 목록: [1238, 1495, 167, 227, 226, 23821, 225, 230, 47991, 112, 13328, 99, 237, 31619, 100, 236, 35975, 112, 31619, 108, 249, 168, 250, 120, 168, 226, 116, 168, 248, 242, 3228, 14628, 968, 6280, 4907]\n",
      "✅ Token 목록: ['20', '25', 'ë', 'ħ', 'Ħ', 'Ġì', 'ĥ', 'Ī', 'íķ', '´', 'Ġç', '¦', 'ı', 'Ġë', '§', 'İ', 'ìĿ', '´', 'Ġë', '°', 'Ľ', 'ì', 'ľ', '¼', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '!!', 'ĠHappy', 'ĠNew', 'ĠYear', '~~']\n",
      "✅ 사람이 읽을 수 있는 Token들: ['20', '25', '�', '�', '�', ' �', '�', '�', '�', '�', ' �', '�', '�', ' �', '�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '!!', ' Happy', ' New', ' Year', '~~']\n",
      "\n",
      " Model: bert-base-multilingual-cased\n",
      "⭐ 입력 문장: 2025년 새해 福 많이 받으세요!! Happy New Year~~\n",
      "✅ Token 수: 17\n",
      "✅ Token ID 목록: [97334, 10954, 9415, 14523, 5933, 47058, 9322, 119185, 24982, 48549, 106, 106, 22678, 10287, 13567, 198, 198]\n",
      "✅ Token 목록: ['2025', '##년', '새', '##해', '福', '많이', '받', '##으', '##세', '##요', '!', '!', 'Happy', 'New', 'Year', '~', '~']\n",
      "✅ 사람이 읽을 수 있는 Token들: ['2025', '##년', '새', '##해', '福', '많이', '받', '##으', '##세', '##요', '!', '!', 'Happy', 'New', 'Year', '~', '~']\n",
      "\n",
      " Model: deepseek-ai/DeepSeek-V3-0324\n",
      "⭐ 입력 문장: 2025년 새해 福 많이 받으세요!! Happy New Year~~\n",
      "✅ Token 수: 16\n",
      "✅ Token ID 목록: [939, 23, 15376, 59157, 9020, 223, 4041, 103313, 49997, 8944, 73527, 6909, 35323, 2010, 10368, 5906]\n",
      "✅ Token 목록: ['202', '5', 'ëħĦ', 'ĠìĥĪ', 'íķ´', 'Ġ', 'ç¦ı', 'Ġë§İìĿ´', 'Ġë°Ľ', 'ìľ¼', 'ìĦ¸ìļĶ', '!!', 'ĠHappy', 'ĠNew', 'ĠYear', '~~']\n",
      "✅ 사람이 읽을 수 있는 Token들: ['202', '5', '년', ' 새', '해', ' ', '福', ' 많이', ' 받', '으', '세요', '!!', ' Happy', ' New', ' Year', '~~']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_with_model(model_name, text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in input_ids]\n",
    "\n",
    "    print(f\"\\n Model: {model_name}\")\n",
    "    print(\"⭐ 입력 문장:\", text)\n",
    "    print(\"✅ Token 수:\", len(input_ids))\n",
    "    print(\"✅ Token ID 목록:\", input_ids)\n",
    "    print(\"✅ Token 목록:\", tokens)\n",
    "    print(\"✅ 사람이 읽을 수 있는 Token들:\", decoded_tokens)\n",
    "\n",
    "# 실험할 문장\n",
    "text = \"2025년 새해 福 많이 받으세요!! Happy New Year~~\"\n",
    "\n",
    "#  tokenizer\n",
    "tokenize_with_model(\"gpt2\", text)\n",
    "\n",
    "# BERT tokenizer\n",
    "tokenize_with_model(\"bert-base-multilingual-cased\", text)\n",
    "\n",
    "# DeepSeek tokenizer\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57065942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Token 수: 15\n",
      "🧬 Token ID 목록: [1323, 20, 12622, 47563, 5650, 26328, 69815, 34792, 6566, 37436, 2618, 27213, 2036, 10343, 7739]\n",
      "🔠 사람이 읽을 수 있는 Token들: ['202', '5', '년', ' 새', '해', ' 福', ' 많이', ' 받', '으', '세요', '!!', ' Happy', ' New', ' Year', '~~']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "text = \"2025년 새해 福 많이 받으세요!! Happy New Year~~\"\n",
    "input_ids = encoding.encode(text)\n",
    "decoded = [encoding.decode([tid]) for tid in input_ids]\n",
    "\n",
    "print(\"✅ Token 수:\", len(input_ids))\n",
    "print(\"🧬 Token ID 목록:\", input_ids)\n",
    "print(\"🔠 사람이 읽을 수 있는 Token들:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484dc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⭐ Model: gpt2\n",
      "✅ 입력 문장: 새해 복 많이 받으세요. 오늘도 운수 좋은 하루 되세요.\n",
      "✅ Token 수: 62\n",
      "✅ Token ID 목록: [168, 225, 230, 47991, 112, 31619, 111, 113, 31619, 100, 236, 35975, 112, 31619, 108, 249, 168, 250, 120, 168, 226, 116, 168, 248, 242, 13, 23821, 246, 97, 167, 232, 246, 167, 237, 226, 23821, 248, 112, 168, 230, 246, 23821, 95, 233, 35975, 222, 220, 47991, 246, 167, 96, 101, 31619, 238, 246, 168, 226, 116, 168, 248, 242, 13]\n",
      "✅ Token 목록: ['ì', 'ĥ', 'Ī', 'íķ', '´', 'Ġë', '³', 'µ', 'Ġë', '§', 'İ', 'ìĿ', '´', 'Ġë', '°', 'Ľ', 'ì', 'ľ', '¼', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.', 'Ġì', 'ĺ', '¤', 'ë', 'Ĭ', 'ĺ', 'ë', 'ı', 'Ħ', 'Ġì', 'ļ', '´', 'ì', 'Ī', 'ĺ', 'Ġì', '¢', 'ĭ', 'ìĿ', 'Ģ', 'Ġ', 'íķ', 'ĺ', 'ë', '£', '¨', 'Ġë', 'Ĳ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ', '.']\n",
      "✅ 사람이 읽을 수 있는 Token들: ['�', '�', '�', '�', '�', ' �', '�', '�', ' �', '�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '.', ' �', '�', '�', '�', '�', '�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', ' �', '�', '�', '�', '�', ' ', '�', '�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', '�', '�', '�', '.']\n",
      "\n",
      "⭐ Model: bert-base-multilingual-cased\n",
      "✅ 입력 문장: 새해 복 많이 받으세요. 오늘도 운수 좋은 하루 되세요.\n",
      "✅ Token 수: 21\n",
      "✅ Token ID 목록: [9415, 14523, 9357, 47058, 9322, 119185, 24982, 48549, 119, 9580, 118762, 12092, 9606, 15891, 79633, 9952, 35866, 9098, 24982, 48549, 119]\n",
      "✅ Token 목록: ['새', '##해', '복', '많이', '받', '##으', '##세', '##요', '.', '오', '##늘', '##도', '운', '##수', '좋은', '하', '##루', '되', '##세', '##요', '.']\n",
      "✅ 사람이 읽을 수 있는 Token들: ['새', '##해', '복', '많이', '받', '##으', '##세', '##요', '.', '오', '##늘', '##도', '운', '##수', '좋은', '하', '##루', '되', '##세', '##요', '.']\n",
      "\n",
      "⭐ Model: deepseek-ai/DeepSeek-V3-0324\n",
      "✅ 입력 문장: 새해 복 많이 받으세요. 오늘도 운수 좋은 하루 되세요.\n",
      "✅ Token 수: 21\n",
      "✅ Token ID 목록: [9825, 233, 9020, 60970, 103313, 49997, 8944, 73527, 16, 30803, 92855, 9260, 59424, 12720, 61912, 7180, 17454, 43624, 34122, 73527, 16]\n",
      "✅ Token 목록: ['ìĥ', 'Ī', 'íķ´', 'Ġë³µ', 'Ġë§İìĿ´', 'Ġë°Ľ', 'ìľ¼', 'ìĦ¸ìļĶ', '.', 'Ġìĺ¤', 'ëĬĺ', 'ëıĦ', 'Ġìļ´', 'ìĪĺ', 'Ġì¢ĭ', 'ìĿĢ', 'Ġíķĺ', 'ë£¨', 'ĠëĲĺ', 'ìĦ¸ìļĶ', '.']\n",
      "✅ 사람이 읽을 수 있는 Token들: ['�', '�', '해', ' 복', ' 많이', ' 받', '으', '세요', '.', ' 오', '늘', '도', ' 운', '수', ' 좋', '은', ' 하', '루', ' 되', '세요', '.']\n",
      "\n",
      "⭐ Model: gpt-4o (tiktoken)\n",
      "✅ 입력 문장: 새해 복 많이 받으세요. 오늘도 운수 좋은 하루 되세요.\n",
      "✅ Token 수: 17\n",
      "✅ Token ID 목록: [52976, 5650, 68238, 69815, 34792, 6566, 37436, 13, 106820, 5827, 47765, 7820, 70135, 141945, 28647, 37436, 13]\n",
      "✅ 사람이 읽을 수 있는 Token들: ['새', '해', ' 복', ' 많이', ' 받', '으', '세요', '.', ' 오늘', '도', ' 운', '수', ' 좋은', ' 하루', ' 되', '세요', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# 비교용 텍스트\n",
    "text = \"새해 복 많이 받으세요. 오늘도 운수 좋은 하루 되세요.\"\n",
    "\n",
    "# Hugging Face 기반 tokenizer 실험 함수\n",
    "def tokenize_with_model(model_name, text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in input_ids]\n",
    "\n",
    "    print(f\"\\n⭐ Model: {model_name}\")\n",
    "    print(\"✅ 입력 문장:\", text)\n",
    "    print(\"✅ Token 수:\", len(input_ids))\n",
    "    print(\"✅ Token ID 목록:\", input_ids)\n",
    "    print(\"✅ Token 목록:\", tokens)\n",
    "    print(\"✅ 사람이 읽을 수 있는 Token들:\", decoded_tokens)\n",
    "\n",
    "# GPT-2\n",
    "tokenize_with_model(\"gpt2\", text)\n",
    "\n",
    "# BERT\n",
    "tokenize_with_model(\"bert-base-multilingual-cased\", text)\n",
    "\n",
    "# DeepSeek\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n",
    "\n",
    "# GPT-4o tokenizer (tiktoken 기반)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "input_ids = encoding.encode(text)\n",
    "decoded = [encoding.decode([tid]) for tid in input_ids]\n",
    "\n",
    "print(\"\\n⭐ Model: gpt-4o (tiktoken)\")\n",
    "print(\"✅ 입력 문장:\", text)\n",
    "print(\"✅ Token 수:\", len(input_ids))\n",
    "print(\"✅ Token ID 목록:\", input_ids)\n",
    "print(\"✅ 사람이 읽을 수 있는 Token들:\", decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
