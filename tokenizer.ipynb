{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885ca0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- deepseek-ai/DeepSeek-V3-0324:\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ë‚´ì¼ë„ ì˜¤ëŠ˜ê³¼ ë˜‘ê°™ì€ í•˜ë£¨ì¼ê¹Œ?\n",
      "ğŸ”¢ Token ê°œìˆ˜: 17\n",
      "ğŸ”  Token ë¦¬ìŠ¤íŠ¸: ['Ã«Ä¤Â´', 'Ã¬Ä¿Â¼', 'Ã«Ä±Ä¦', 'Ä Ã¬ÄºÂ¤', 'Ã«Ä¬Äº', 'ÃªÂ³Â¼', 'Ä Ã«', 'Äº', 'Ä³', 'ÃªÂ°', 'Ä»', 'Ã¬Ä¿Ä¢', 'Ä Ã­Ä·Äº', 'Ã«Â£Â¨', 'Ã¬Ä¿Â¼', 'ÃªÂ¹Ä®', '?']\n",
      "ğŸ§¬ Input IDs: [35076, 14304, 9260, 30803, 92855, 9862, 1525, 249, 242, 4598, 250, 7180, 17454, 43624, 14304, 30939, 33]\n",
      "\n",
      "- Qwen/QwQ-32B:\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ë‚´ì¼ë„ ì˜¤ëŠ˜ê³¼ ë˜‘ê°™ì€ í•˜ë£¨ì¼ê¹Œ?\n",
      "ğŸ”¢ Token ê°œìˆ˜: 14\n",
      "ğŸ”  Token ë¦¬ìŠ¤íŠ¸: ['Ã«Ä¤Â´', 'Ã¬Ä¿Â¼', 'Ã«Ä±Ä¦', 'Ä Ã¬ÄºÂ¤Ã«Ä¬Äº', 'ÃªÂ³Â¼', 'Ä Ã«Äº', 'Ä³', 'ÃªÂ°Ä»', 'Ã¬Ä¿Ä¢', 'Ä Ã­Ä·Äº', 'Ã«Â£Â¨', 'Ã¬Ä¿Â¼', 'ÃªÂ¹Ä®', '?']\n",
      "ğŸ§¬ Input IDs: [95218, 32077, 47985, 133857, 53680, 125639, 239, 131380, 33704, 53900, 126746, 32077, 124667, 30]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_with_model(model_name, text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\n- {model_name}:\")\n",
    "    print(\"âœ… ì…ë ¥ ë¬¸ì¥:\", text)\n",
    "    print(\"ğŸ”¢ Token ê°œìˆ˜:\", len(input_ids))\n",
    "    print(\"ğŸ”  Token ë¦¬ìŠ¤íŠ¸:\", tokens)\n",
    "    print(\"ğŸ§¬ Input IDs:\", input_ids)\n",
    "\n",
    "text = \"ë‚´ì¼ë„ ì˜¤ëŠ˜ê³¼ ë˜‘ê°™ì€ í•˜ë£¨ì¼ê¹Œ?\"\n",
    "\n",
    "# ì›í•˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ì—¬ê¸°ì— ì…ë ¥\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n",
    "tokenize_with_model(\"Qwen/QwQ-32B\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d2ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- deepseek-ai/DeepSeek-V3-0324:\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ë‚´ì¼ë„ ì˜¤ëŠ˜ê³¼ ë˜‘ê°™ì€ í•˜ë£¨ì¼ê¹Œ? ì˜¤ëŠ˜ë³´ë‹¤ ì¦ê±°ì› ìœ¼ë©´ ì¢‹ê² ë‹¤!!^^\n",
      "ğŸ”§ Tokenizer ì¢…ë¥˜: LlamaTokenizerFast\n",
      "ğŸ”¢ Token ê°œìˆ˜: 31\n",
      "ğŸ”  Token ë¦¬ìŠ¤íŠ¸: ['Ã«Ä¤Â´', 'Ã¬Ä¿Â¼', 'Ã«Ä±Ä¦', 'Ä Ã¬ÄºÂ¤', 'Ã«Ä¬Äº', 'ÃªÂ³Â¼', 'Ä Ã«', 'Äº', 'Ä³', 'ÃªÂ°', 'Ä»', 'Ã¬Ä¿Ä¢', 'Ä Ã­Ä·Äº', 'Ã«Â£Â¨', 'Ã¬Ä¿Â¼', 'ÃªÂ¹Ä®', '?', 'Ä Ã¬ÄºÂ¤', 'Ã«Ä¬Äº', 'Ã«Â³Â´Ã«Ä­Â¤', 'Ä Ã¬Â¦', 'Ä²', 'ÃªÂ±Â°', 'Ã¬Ä½', 'Å‚', 'Ã¬Ä¾Â¼Ã«Â©Â´', 'Ä Ã¬Â¢Ä­', 'ÃªÂ²Å‚', 'Ã«Ä­Â¤', '!!', '^^']\n",
      "ğŸ§¬ Input IDs: [35076, 14304, 9260, 30803, 92855, 9862, 1525, 249, 242, 4598, 250, 7180, 17454, 43624, 14304, 30939, 33, 30803, 92855, 62811, 37589, 241, 28058, 10204, 257, 80726, 61912, 70096, 3874, 6909, 29670]\n",
      "ğŸ” ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['ë‚´', 'ì¼', 'ë„', ' ì˜¤', 'ëŠ˜', 'ê³¼', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ì€', ' í•˜', 'ë£¨', 'ì¼', 'ê¹Œ', '?', ' ì˜¤', 'ëŠ˜', 'ë³´ë‹¤', ' ï¿½', 'ï¿½', 'ê±°', 'ï¿½', 'ï¿½', 'ìœ¼ë©´', ' ì¢‹', 'ê² ', 'ë‹¤', '!!', '^^']\n",
      "ğŸ’° ì˜ˆìƒ í† í° ë¹„ìš© (USD): $0.00006200\n",
      "\n",
      "- Qwen/QwQ-32B:\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ë‚´ì¼ë„ ì˜¤ëŠ˜ê³¼ ë˜‘ê°™ì€ í•˜ë£¨ì¼ê¹Œ? ì˜¤ëŠ˜ë³´ë‹¤ ì¦ê±°ì› ìœ¼ë©´ ì¢‹ê² ë‹¤!!^^\n",
      "ğŸ”§ Tokenizer ì¢…ë¥˜: Qwen2TokenizerFast\n",
      "ğŸ”¢ Token ê°œìˆ˜: 25\n",
      "ğŸ”  Token ë¦¬ìŠ¤íŠ¸: ['Ã«Ä¤Â´', 'Ã¬Ä¿Â¼', 'Ã«Ä±Ä¦', 'Ä Ã¬ÄºÂ¤Ã«Ä¬Äº', 'ÃªÂ³Â¼', 'Ä Ã«Äº', 'Ä³', 'ÃªÂ°Ä»', 'Ã¬Ä¿Ä¢', 'Ä Ã­Ä·Äº', 'Ã«Â£Â¨', 'Ã¬Ä¿Â¼', 'ÃªÂ¹Ä®', '?', 'Ä Ã¬ÄºÂ¤Ã«Ä¬Äº', 'Ã«Â³Â´Ã«Ä­Â¤', 'Ä Ã¬Â¦', 'Ä²', 'ÃªÂ±Â°', 'Ã¬Ä½Å‚', 'Ã¬Ä¾Â¼Ã«Â©Â´', 'Ä Ã¬Â¢Ä­', 'ÃªÂ²Å‚Ã«Ä­Â¤', '!!', '^^']\n",
      "ğŸ§¬ Input IDs: [95218, 32077, 47985, 133857, 53680, 125639, 239, 131380, 33704, 53900, 126746, 32077, 124667, 30, 133857, 129885, 132376, 238, 92192, 144035, 89940, 125953, 127816, 2928, 21876]\n",
      "ğŸ” ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['ë‚´', 'ì¼', 'ë„', ' ì˜¤ëŠ˜', 'ê³¼', ' ï¿½', 'ï¿½', 'ê°™', 'ì€', ' í•˜', 'ë£¨', 'ì¼', 'ê¹Œ', '?', ' ì˜¤ëŠ˜', 'ë³´ë‹¤', ' ï¿½', 'ï¿½', 'ê±°', 'ì› ', 'ìœ¼ë©´', ' ì¢‹', 'ê² ë‹¤', '!!', '^^']\n",
      "ğŸ’° ì˜ˆìƒ í† í° ë¹„ìš© (USD): $0.00005000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_with_model(model_name, text, cost_per_token=0.000002):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in input_ids]\n",
    "    estimated_cost = len(input_ids) * cost_per_token\n",
    "\n",
    "    print(f\"\\n- {model_name}:\")\n",
    "    print(\"âœ… ì…ë ¥ ë¬¸ì¥:\", text)\n",
    "    print(\"ğŸ”§ Tokenizer ì¢…ë¥˜:\", tokenizer.__class__.__name__)\n",
    "    print(\"ğŸ”¢ Token ê°œìˆ˜:\", len(input_ids))\n",
    "    print(\"ğŸ”  Token ë¦¬ìŠ¤íŠ¸:\", tokens)\n",
    "    print(\"ğŸ§¬ Input IDs:\", input_ids)\n",
    "    print(\"ğŸ” ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤:\", decoded_tokens)\n",
    "    print(f\"ğŸ’° ì˜ˆìƒ í† í° ë¹„ìš© (USD): ${estimated_cost:.8f}\")\n",
    "\n",
    "# ê³µí†µ ì…ë ¥ ë¬¸ì¥\n",
    "text = \"ë‚´ì¼ë„ ì˜¤ëŠ˜ê³¼ ë˜‘ê°™ì€ í•˜ë£¨ì¼ê¹Œ? ì˜¤ëŠ˜ë³´ë‹¤ ì¦ê±°ì› ìœ¼ë©´ ì¢‹ê² ë‹¤!!^^\"\n",
    "\n",
    "# ë¹„êµí•  ëª¨ë¸\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n",
    "tokenize_with_model(\"Qwen/QwQ-32B\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a888f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model: gpt2\n",
      "â­ ì…ë ¥ ë¬¸ì¥: 2025ë…„ ìƒˆí•´ ç¦ ë§ì´ ë°›ìœ¼ì„¸ìš”!! Happy New Year~~\n",
      "âœ… Token ìˆ˜: 35\n",
      "âœ… Token ID ëª©ë¡: [1238, 1495, 167, 227, 226, 23821, 225, 230, 47991, 112, 13328, 99, 237, 31619, 100, 236, 35975, 112, 31619, 108, 249, 168, 250, 120, 168, 226, 116, 168, 248, 242, 3228, 14628, 968, 6280, 4907]\n",
      "âœ… Token ëª©ë¡: ['20', '25', 'Ã«', 'Ä§', 'Ä¦', 'Ä Ã¬', 'Ä¥', 'Äª', 'Ã­Ä·', 'Â´', 'Ä Ã§', 'Â¦', 'Ä±', 'Ä Ã«', 'Â§', 'Ä°', 'Ã¬Ä¿', 'Â´', 'Ä Ã«', 'Â°', 'Ä½', 'Ã¬', 'Ä¾', 'Â¼', 'Ã¬', 'Ä¦', 'Â¸', 'Ã¬', 'Ä¼', 'Ä¶', '!!', 'Ä Happy', 'Ä New', 'Ä Year', '~~']\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['20', '25', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', '!!', ' Happy', ' New', ' Year', '~~']\n",
      "\n",
      " Model: bert-base-multilingual-cased\n",
      "â­ ì…ë ¥ ë¬¸ì¥: 2025ë…„ ìƒˆí•´ ç¦ ë§ì´ ë°›ìœ¼ì„¸ìš”!! Happy New Year~~\n",
      "âœ… Token ìˆ˜: 17\n",
      "âœ… Token ID ëª©ë¡: [97334, 10954, 9415, 14523, 5933, 47058, 9322, 119185, 24982, 48549, 106, 106, 22678, 10287, 13567, 198, 198]\n",
      "âœ… Token ëª©ë¡: ['2025', '##ë…„', 'ìƒˆ', '##í•´', 'ç¦', 'ë§ì´', 'ë°›', '##ìœ¼', '##ì„¸', '##ìš”', '!', '!', 'Happy', 'New', 'Year', '~', '~']\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['2025', '##ë…„', 'ìƒˆ', '##í•´', 'ç¦', 'ë§ì´', 'ë°›', '##ìœ¼', '##ì„¸', '##ìš”', '!', '!', 'Happy', 'New', 'Year', '~', '~']\n",
      "\n",
      " Model: deepseek-ai/DeepSeek-V3-0324\n",
      "â­ ì…ë ¥ ë¬¸ì¥: 2025ë…„ ìƒˆí•´ ç¦ ë§ì´ ë°›ìœ¼ì„¸ìš”!! Happy New Year~~\n",
      "âœ… Token ìˆ˜: 16\n",
      "âœ… Token ID ëª©ë¡: [939, 23, 15376, 59157, 9020, 223, 4041, 103313, 49997, 8944, 73527, 6909, 35323, 2010, 10368, 5906]\n",
      "âœ… Token ëª©ë¡: ['202', '5', 'Ã«Ä§Ä¦', 'Ä Ã¬Ä¥Äª', 'Ã­Ä·Â´', 'Ä ', 'Ã§Â¦Ä±', 'Ä Ã«Â§Ä°Ã¬Ä¿Â´', 'Ä Ã«Â°Ä½', 'Ã¬Ä¾Â¼', 'Ã¬Ä¦Â¸Ã¬Ä¼Ä¶', '!!', 'Ä Happy', 'Ä New', 'Ä Year', '~~']\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['202', '5', 'ë…„', ' ìƒˆ', 'í•´', ' ', 'ç¦', ' ë§ì´', ' ë°›', 'ìœ¼', 'ì„¸ìš”', '!!', ' Happy', ' New', ' Year', '~~']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_with_model(model_name, text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in input_ids]\n",
    "\n",
    "    print(f\"\\n Model: {model_name}\")\n",
    "    print(\"â­ ì…ë ¥ ë¬¸ì¥:\", text)\n",
    "    print(\"âœ… Token ìˆ˜:\", len(input_ids))\n",
    "    print(\"âœ… Token ID ëª©ë¡:\", input_ids)\n",
    "    print(\"âœ… Token ëª©ë¡:\", tokens)\n",
    "    print(\"âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤:\", decoded_tokens)\n",
    "\n",
    "# ì‹¤í—˜í•  ë¬¸ì¥\n",
    "text = \"2025ë…„ ìƒˆí•´ ç¦ ë§ì´ ë°›ìœ¼ì„¸ìš”!! Happy New Year~~\"\n",
    "\n",
    "#  tokenizer\n",
    "tokenize_with_model(\"gpt2\", text)\n",
    "\n",
    "# BERT tokenizer\n",
    "tokenize_with_model(\"bert-base-multilingual-cased\", text)\n",
    "\n",
    "# DeepSeek tokenizer\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57065942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Token ìˆ˜: 15\n",
      "ğŸ§¬ Token ID ëª©ë¡: [1323, 20, 12622, 47563, 5650, 26328, 69815, 34792, 6566, 37436, 2618, 27213, 2036, 10343, 7739]\n",
      "ğŸ”  ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['202', '5', 'ë…„', ' ìƒˆ', 'í•´', ' ç¦', ' ë§ì´', ' ë°›', 'ìœ¼', 'ì„¸ìš”', '!!', ' Happy', ' New', ' Year', '~~']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "text = \"2025ë…„ ìƒˆí•´ ç¦ ë§ì´ ë°›ìœ¼ì„¸ìš”!! Happy New Year~~\"\n",
    "input_ids = encoding.encode(text)\n",
    "decoded = [encoding.decode([tid]) for tid in input_ids]\n",
    "\n",
    "print(\"âœ… Token ìˆ˜:\", len(input_ids))\n",
    "print(\"ğŸ§¬ Token ID ëª©ë¡:\", input_ids)\n",
    "print(\"ğŸ”  ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484dc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â­ Model: gpt2\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ìƒˆí•´ ë³µ ë§ì´ ë°›ìœ¼ì„¸ìš”. ì˜¤ëŠ˜ë„ ìš´ìˆ˜ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.\n",
      "âœ… Token ìˆ˜: 62\n",
      "âœ… Token ID ëª©ë¡: [168, 225, 230, 47991, 112, 31619, 111, 113, 31619, 100, 236, 35975, 112, 31619, 108, 249, 168, 250, 120, 168, 226, 116, 168, 248, 242, 13, 23821, 246, 97, 167, 232, 246, 167, 237, 226, 23821, 248, 112, 168, 230, 246, 23821, 95, 233, 35975, 222, 220, 47991, 246, 167, 96, 101, 31619, 238, 246, 168, 226, 116, 168, 248, 242, 13]\n",
      "âœ… Token ëª©ë¡: ['Ã¬', 'Ä¥', 'Äª', 'Ã­Ä·', 'Â´', 'Ä Ã«', 'Â³', 'Âµ', 'Ä Ã«', 'Â§', 'Ä°', 'Ã¬Ä¿', 'Â´', 'Ä Ã«', 'Â°', 'Ä½', 'Ã¬', 'Ä¾', 'Â¼', 'Ã¬', 'Ä¦', 'Â¸', 'Ã¬', 'Ä¼', 'Ä¶', '.', 'Ä Ã¬', 'Äº', 'Â¤', 'Ã«', 'Ä¬', 'Äº', 'Ã«', 'Ä±', 'Ä¦', 'Ä Ã¬', 'Ä¼', 'Â´', 'Ã¬', 'Äª', 'Äº', 'Ä Ã¬', 'Â¢', 'Ä­', 'Ã¬Ä¿', 'Ä¢', 'Ä ', 'Ã­Ä·', 'Äº', 'Ã«', 'Â£', 'Â¨', 'Ä Ã«', 'Ä²', 'Äº', 'Ã¬', 'Ä¦', 'Â¸', 'Ã¬', 'Ä¼', 'Ä¶', '.']\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', '.', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', ' ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', 'ï¿½', '.']\n",
      "\n",
      "â­ Model: bert-base-multilingual-cased\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ìƒˆí•´ ë³µ ë§ì´ ë°›ìœ¼ì„¸ìš”. ì˜¤ëŠ˜ë„ ìš´ìˆ˜ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.\n",
      "âœ… Token ìˆ˜: 21\n",
      "âœ… Token ID ëª©ë¡: [9415, 14523, 9357, 47058, 9322, 119185, 24982, 48549, 119, 9580, 118762, 12092, 9606, 15891, 79633, 9952, 35866, 9098, 24982, 48549, 119]\n",
      "âœ… Token ëª©ë¡: ['ìƒˆ', '##í•´', 'ë³µ', 'ë§ì´', 'ë°›', '##ìœ¼', '##ì„¸', '##ìš”', '.', 'ì˜¤', '##ëŠ˜', '##ë„', 'ìš´', '##ìˆ˜', 'ì¢‹ì€', 'í•˜', '##ë£¨', 'ë˜', '##ì„¸', '##ìš”', '.']\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['ìƒˆ', '##í•´', 'ë³µ', 'ë§ì´', 'ë°›', '##ìœ¼', '##ì„¸', '##ìš”', '.', 'ì˜¤', '##ëŠ˜', '##ë„', 'ìš´', '##ìˆ˜', 'ì¢‹ì€', 'í•˜', '##ë£¨', 'ë˜', '##ì„¸', '##ìš”', '.']\n",
      "\n",
      "â­ Model: deepseek-ai/DeepSeek-V3-0324\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ìƒˆí•´ ë³µ ë§ì´ ë°›ìœ¼ì„¸ìš”. ì˜¤ëŠ˜ë„ ìš´ìˆ˜ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.\n",
      "âœ… Token ìˆ˜: 21\n",
      "âœ… Token ID ëª©ë¡: [9825, 233, 9020, 60970, 103313, 49997, 8944, 73527, 16, 30803, 92855, 9260, 59424, 12720, 61912, 7180, 17454, 43624, 34122, 73527, 16]\n",
      "âœ… Token ëª©ë¡: ['Ã¬Ä¥', 'Äª', 'Ã­Ä·Â´', 'Ä Ã«Â³Âµ', 'Ä Ã«Â§Ä°Ã¬Ä¿Â´', 'Ä Ã«Â°Ä½', 'Ã¬Ä¾Â¼', 'Ã¬Ä¦Â¸Ã¬Ä¼Ä¶', '.', 'Ä Ã¬ÄºÂ¤', 'Ã«Ä¬Äº', 'Ã«Ä±Ä¦', 'Ä Ã¬Ä¼Â´', 'Ã¬ÄªÄº', 'Ä Ã¬Â¢Ä­', 'Ã¬Ä¿Ä¢', 'Ä Ã­Ä·Äº', 'Ã«Â£Â¨', 'Ä Ã«Ä²Äº', 'Ã¬Ä¦Â¸Ã¬Ä¼Ä¶', '.']\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['ï¿½', 'ï¿½', 'í•´', ' ë³µ', ' ë§ì´', ' ë°›', 'ìœ¼', 'ì„¸ìš”', '.', ' ì˜¤', 'ëŠ˜', 'ë„', ' ìš´', 'ìˆ˜', ' ì¢‹', 'ì€', ' í•˜', 'ë£¨', ' ë˜', 'ì„¸ìš”', '.']\n",
      "\n",
      "â­ Model: gpt-4o (tiktoken)\n",
      "âœ… ì…ë ¥ ë¬¸ì¥: ìƒˆí•´ ë³µ ë§ì´ ë°›ìœ¼ì„¸ìš”. ì˜¤ëŠ˜ë„ ìš´ìˆ˜ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.\n",
      "âœ… Token ìˆ˜: 17\n",
      "âœ… Token ID ëª©ë¡: [52976, 5650, 68238, 69815, 34792, 6566, 37436, 13, 106820, 5827, 47765, 7820, 70135, 141945, 28647, 37436, 13]\n",
      "âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤: ['ìƒˆ', 'í•´', ' ë³µ', ' ë§ì´', ' ë°›', 'ìœ¼', 'ì„¸ìš”', '.', ' ì˜¤ëŠ˜', 'ë„', ' ìš´', 'ìˆ˜', ' ì¢‹ì€', ' í•˜ë£¨', ' ë˜', 'ì„¸ìš”', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# ë¹„êµìš© í…ìŠ¤íŠ¸\n",
    "text = \"ìƒˆí•´ ë³µ ë§ì´ ë°›ìœ¼ì„¸ìš”. ì˜¤ëŠ˜ë„ ìš´ìˆ˜ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.\"\n",
    "\n",
    "# Hugging Face ê¸°ë°˜ tokenizer ì‹¤í—˜ í•¨ìˆ˜\n",
    "def tokenize_with_model(model_name, text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in input_ids]\n",
    "\n",
    "    print(f\"\\nâ­ Model: {model_name}\")\n",
    "    print(\"âœ… ì…ë ¥ ë¬¸ì¥:\", text)\n",
    "    print(\"âœ… Token ìˆ˜:\", len(input_ids))\n",
    "    print(\"âœ… Token ID ëª©ë¡:\", input_ids)\n",
    "    print(\"âœ… Token ëª©ë¡:\", tokens)\n",
    "    print(\"âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤:\", decoded_tokens)\n",
    "\n",
    "# GPT-2\n",
    "tokenize_with_model(\"gpt2\", text)\n",
    "\n",
    "# BERT\n",
    "tokenize_with_model(\"bert-base-multilingual-cased\", text)\n",
    "\n",
    "# DeepSeek\n",
    "tokenize_with_model(\"deepseek-ai/DeepSeek-V3-0324\", text)\n",
    "\n",
    "# GPT-4o tokenizer (tiktoken ê¸°ë°˜)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "input_ids = encoding.encode(text)\n",
    "decoded = [encoding.decode([tid]) for tid in input_ids]\n",
    "\n",
    "print(\"\\nâ­ Model: gpt-4o (tiktoken)\")\n",
    "print(\"âœ… ì…ë ¥ ë¬¸ì¥:\", text)\n",
    "print(\"âœ… Token ìˆ˜:\", len(input_ids))\n",
    "print(\"âœ… Token ID ëª©ë¡:\", input_ids)\n",
    "print(\"âœ… ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” Tokenë“¤:\", decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
